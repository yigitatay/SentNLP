{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Flatten, Bidirectional, \\\n",
    "                            Dropout, Embedding, Conv2D, MaxPool2D, Reshape, \\\n",
    "                            TimeDistributed, Activation, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "# TO RUN ON GPU, UNCOMMENT\n",
    "# import tensorflow as tf\n",
    "# config = tf.compat.v1.ConfigProto(device_count = {'GPU':2})\n",
    "# sess = tf.compat.v1.Session(config=config)\n",
    "# tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data into a pandas dataframe\n",
    "train_file_path = \"/Users/yigitatay/Desktop/SentNLP/data/train.csv\"\n",
    "df = pd.read_csv(train_file_path)\n",
    "test_file_path = \"/Users/yigitatay/Desktop/SentNLP/data/test.csv\"\n",
    "df_test = pd.read_csv(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate labels and data\n",
    "texts = df[\"text\"]\n",
    "selected_texts = df[\"selected_text\"]\n",
    "sentiments = df[\"sentiment\"]\n",
    "# lists to hold text, sentiment\n",
    "train_list = []\n",
    "train_sentiment_list = []\n",
    "# a list to hold the labels\n",
    "label_list = []\n",
    "for text, data, label in zip(texts, sentiments, selected_texts):\n",
    "    text = str(text).split()\n",
    "    train_list.append(text)\n",
    "    label = str(label).split()\n",
    "    label_list.append(label)\n",
    "    train_sentiment_list.append(data)\n",
    "    \n",
    "# separate labels and data\n",
    "texts = df_test[\"text\"]\n",
    "sentiments = df_test[\"sentiment\"]\n",
    "# lists to hold text, sentiment\n",
    "test_list = []\n",
    "test_list_not_tokenized = []\n",
    "test_sentiment_list = []\n",
    "for text, data in zip(texts, sentiments):\n",
    "    test_list_not_tokenized.append(text)\n",
    "    text = str(text).split()\n",
    "    test_list.append(text)\n",
    "    test_sentiment_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27481\n"
     ]
    }
   ],
   "source": [
    "print(len(train_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD THE WORD VECTORS FROM THE PRETRAINED WORD2VEC MODEL\n",
    "model = Word2Vec.load(\"word2vec_checkpoints/word2vec_80.model\")\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum number of words in a tweet in this dataset is 33, so 40 would be a good standard for sentence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UNCOMMENT TO RECREATE THE DATASET IF IT'S NOT SAVED\n",
    "\n",
    "# spell = SpellChecker()\n",
    "# input_dataset = np.zeros((27481, 40, 80), dtype=float)\n",
    "# for i, sentence in enumerate(train_list):\n",
    "#     list_to_add = np.array([])\n",
    "#     for word in sentence:\n",
    "#         # if it's a link, add a vector of 0's\n",
    "#         if word[0:4] == \"http\":\n",
    "#             list_to_add = np.append(list_to_add, np.zeros(80))\n",
    "#             continue\n",
    "#         if \"****\" in word:\n",
    "#             word = word.replace('****', 'censored')\n",
    "#         str1 = ''\n",
    "#         str2 = ''\n",
    "#         switch = False\n",
    "#         for char in word:\n",
    "#             if char.isalpha() or char==\"`\" or char==\"-\":\n",
    "#                 if switch:\n",
    "#                     str2 = str2 + char\n",
    "#                 else:\n",
    "#                     str1 = str1 + char\n",
    "#             else:\n",
    "#                 switch = True\n",
    "#         count = 1\n",
    "#         tempChar = ''\n",
    "#         newStr = ''\n",
    "#         for char in str1:\n",
    "#             if char == tempChar:\n",
    "#                 count += 1\n",
    "#             tempChar = char\n",
    "#             if count < 3:\n",
    "#                 newStr += char\n",
    "#             else:\n",
    "#                 newStr = newStr[:-1]\n",
    "#                 continue\n",
    "#         count = 1\n",
    "#         tempChar = ''\n",
    "#         newStr2 = ''\n",
    "#         for char in str2:\n",
    "#             if char == tempChar:\n",
    "#                 count += 1\n",
    "#             tempChar = char\n",
    "#             if count < 3:\n",
    "#                 newStr2 += char\n",
    "#             else:\n",
    "#                 newStr2 = newStr[:-1]\n",
    "#                 continue\n",
    "                \n",
    "#         if newStr != \"\" and newStr != \"`\" and newStr != \"-\":\n",
    "#             list_to_add = np.append(list_to_add, np.array(word_vectors[spell.correction(newStr.lower())]))\n",
    "#         if newStr2 != \"\" and newStr2 != \"`\" and newStr2 != \"-\":\n",
    "#             list_to_add = np.append(list_to_add, word_vectors[spell.correction(newStr2.lower())])\n",
    "#     while list_to_add.shape[0] != 3200:\n",
    "#         list_to_add = np.append(list_to_add, np.zeros(80))\n",
    "#     list_to_add = np.reshape(list_to_add, [40, 80])\n",
    "#     input_dataset[i] = list_to_add\n",
    "#     if i % 100 == 0:\n",
    "#         print(\"Addition number: %d\" % i)\n",
    "# np.save(\"input_without_sentiment\", input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive tweets: 8582\n",
      "Negative tweets: 7781\n",
      "Neutral tweets: 19700\n",
      "Total non-neutral tweets: 16363\n"
     ]
    }
   ],
   "source": [
    "## TO FIND THE COUNTS OF POSITIVE, NEGATIVE AND NEUTRAL TWEETS\n",
    "count_pos = 0\n",
    "count_neg = 0\n",
    "count_neut = 0\n",
    "for sent in train_sentiment_list:\n",
    "    if sent == \"positive\":\n",
    "        count_pos += 1\n",
    "    if sent == \"negative\":\n",
    "        count_neg += 1\n",
    "    else:\n",
    "        count_neut += 1\n",
    "print(\"Positive tweets: \" + str(count_pos))\n",
    "print(\"Negative tweets: \" + str(count_neg))\n",
    "print(\"Neutral tweets: \" + str(count_neut))\n",
    "print(\"Total non-neutral tweets: \" + str(count_pos + count_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO LOAD THE DATASET (WITHOUT SENTIMENT)\n",
    "# train_array = np.load(\"input_without_sentiment.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UNCOMMENT TO CREATE THE DATASET WITH THE SENTIMENTS\n",
    "# Concatanate the sentiment on each word\n",
    "# # pos_array = np.ones((40, 80), dtype=float)\n",
    "# # neg_array = -np.ones((40, 80), dtype=float)\n",
    "# # neutral_array = np.zeros((40, 80), dtype=float)\n",
    "\n",
    "# train_array_with_sent = np.zeros((train_array.shape[0], 40, 80, 2))\n",
    "# pos_neg_array = \n",
    "# for i in range(train_array.shape[0]):\n",
    "#     if train_sentiment_list[i] == \"positive\":\n",
    "#         result = np.dstack((train_array[i], neutral_array))\n",
    "#     elif train_sentiment_list[i] == \"negative\":\n",
    "#         result = np.dstack((train_array[i], neutral_array))\n",
    "#     else:\n",
    "#         result = np.dstack((train_array[i], neutral_array))\n",
    "#     train_array_with_sent[i] = result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19700, 40, 80)\n",
      "(16363, 40, 80)\n"
     ]
    }
   ],
   "source": [
    "## CREATE DATASET FOR POS-NEG and NEUTRAL SEPARATELY\n",
    "pos_neg_array = np.zeros((count_pos+count_neg, 40, 80))\n",
    "neutral_array = np.zeros((count_neut, 40, 80))\n",
    "pos_neg_index = 0\n",
    "neut_index = 0\n",
    "for i in range(train_array.shape[0]):\n",
    "    if train_sentiment_list[i] == \"positive\" or train_sentiment_list[i] == \"negative\":\n",
    "        pos_neg_array[pos_neg_index] = train_array[i]\n",
    "        pos_neg_index += 1\n",
    "    else:\n",
    "        neutral_array[neut_index] = train_array[i]\n",
    "        neut_index += 1\n",
    "print(neutral_array.shape)\n",
    "print(pos_neg_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"input_with_sentiment\", train_array_with_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UNCOMMENT TO CREATE THE LABELS FOR TRAINING\n",
    "# label_train = np.zeros((len(train_list), 40), dtype=np.float32)\n",
    "# label_train.fill(.2)\n",
    "# for (num, item), label in zip(enumerate(train_list), label_list):\n",
    "#     loc = 0\n",
    "#     for i, word in enumerate(item):\n",
    "#         if loc != len(label) and (label[loc] == word[0:len(label[loc])] or label[loc] == word[-len(label[loc]):]):\n",
    "#             label_train[num][i] = .8\n",
    "#             loc += 1\n",
    "#         else:\n",
    "#             loc = 0\n",
    "# np.save(\"labels\", label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATING LABELS FOR POS-NEG and NEUTRAL SEPARATELY\n",
    "\n",
    "# The reason to fill with 0.2s and 0.8s instead of 0s and 1s is to\n",
    "# make the training better and prevent vanishing gradients\n",
    "label_train = np.zeros((len(train_list), 40), dtype=np.float32)\n",
    "label_train.fill(0.2)\n",
    "for (num, item), label in zip(enumerate(train_list), label_list):\n",
    "    loc = 0\n",
    "    for i, word in enumerate(item):\n",
    "        if loc != len(label) and (label[loc] == word[0:len(label[loc])] or label[loc] == word[-len(label[loc]):]):\n",
    "            label_train[num][i] = 0.8\n",
    "            loc += 1\n",
    "        else:\n",
    "            loc = 0\n",
    "label_pos_neg = np.zeros((count_pos+count_neg, 40), dtype=np.float32)\n",
    "label_neut = np.zeros((count_neut, 40), dtype=np.float32)\n",
    "label_pos_neg.fill(0.2)\n",
    "label_neut.fill(0.2)\n",
    "neut_index = 0\n",
    "pos_neg_index = 0\n",
    "for i in range(len(label_list)):\n",
    "    if train_sentiment_list[i] == \"positive\" or train_sentiment_list[i] == \"negative\":\n",
    "        label_pos_neg[pos_neg_index] = label_train[i]\n",
    "        pos_neg_index += 1\n",
    "    else:\n",
    "        label_neut[neut_index] = label_train[i]\n",
    "        neut_index += 1\n",
    "np.save(\"labels_pos_neg\", label_pos_neg)\n",
    "np.save(\"labels_neut\", label_neut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOME OTHER SAVED ARRAYS WITHOUT POS-NEG / NEUT SEPARATION\n",
    "# labels = np.load(\"labels.npy\")\n",
    "# train = np.load(\"input_wit_sentiment.npy\")\n",
    "# train = np.load(\"input_without_sentiment.npy\")\n",
    "\n",
    "pos_neg_labels = np.load(\"labels_pos_neg.npy\")\n",
    "neut_labels = np.load(\"labels_neut.npy\")\n",
    "\n",
    "pos_neg_train = pos_neg_array\n",
    "neut_train = neutral_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_24 (Bidirectio multiple                  28928     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "bidirectional_25 (Bidirectio multiple                  24832     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "bidirectional_26 (Bidirectio multiple                  24832     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "bidirectional_27 (Bidirectio multiple                  13600     \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  64040     \n",
      "=================================================================\n",
      "Total params: 156,232\n",
      "Trainable params: 156,232\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17730 samples, validate on 1970 samples\n",
      "Epoch 1/6\n",
      "17730/17730 [==============================] - 610s 34ms/sample - loss: 0.5186 - binary_crossentropy: 0.5186 - accuracy: 0.0000e+00 - val_loss: 0.5008 - val_binary_crossentropy: 0.5008 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/6\n",
      "17730/17730 [==============================] - 573s 32ms/sample - loss: 0.5092 - binary_crossentropy: 0.5092 - accuracy: 0.0000e+00 - val_loss: 0.5005 - val_binary_crossentropy: 0.5005 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/6\n",
      "17730/17730 [==============================] - 552s 31ms/sample - loss: 0.5083 - binary_crossentropy: 0.5083 - accuracy: 0.0000e+00 - val_loss: 0.5005 - val_binary_crossentropy: 0.5005 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/6\n",
      "17730/17730 [==============================] - 568s 32ms/sample - loss: 0.5079 - binary_crossentropy: 0.5079 - accuracy: 0.0000e+00 - val_loss: 0.5005 - val_binary_crossentropy: 0.5005 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/6\n",
      "17730/17730 [==============================] - 582s 33ms/sample - loss: 0.5076 - binary_crossentropy: 0.5076 - accuracy: 0.0000e+00 - val_loss: 0.5004 - val_binary_crossentropy: 0.5004 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/6\n",
      "17730/17730 [==============================] - 562s 32ms/sample - loss: 0.5073 - binary_crossentropy: 0.5073 - accuracy: 0.0000e+00 - val_loss: 0.5004 - val_binary_crossentropy: 0.5004 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(20, return_sequences=True)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(40, activation='relu'))\n",
    "\n",
    "# A CONVOLUTIONAL MODEL THAT DOESN'T WORK AS WELL\n",
    "# model.add(Input(shape=(40, 80, 2), name='model_input'))\n",
    "# model.add(Conv2D(filters=8, kernel_size=3, strides=2, padding='same', activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(filters=16, kernel_size=3, strides=2, padding='same', activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(40, activation='relu'))\n",
    "\n",
    "model.build(input_shape=(None, 40, 80))\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=['binary_crossentropy', 'accuracy'])\n",
    "\n",
    "model.fit(neut_train, neut_labels,\n",
    "          batch_size=2,\n",
    "          epochs=6, \n",
    "          validation_split=0.1)\n",
    "model.save(\"neut_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UNCOMMENT TO RECREATE THE TEST DATASET IF IT'S NOT SAVED\n",
    "\n",
    "# spell = SpellChecker()\n",
    "# test_dataset = np.zeros((len(test_list), 40, 80), dtype=float)\n",
    "# for i, sentence in enumerate(test_list):\n",
    "#     list_to_add = np.array([])\n",
    "#     for word in sentence:\n",
    "#         # if it's a link, add a vector of 0's\n",
    "#         if word[0:4] == \"http\":\n",
    "#             list_to_add = np.append(list_to_add, np.zeros(80))\n",
    "#             continue\n",
    "#         if \"****\" in word:\n",
    "#             word = word.replace('****', 'censored')\n",
    "#         str1 = ''\n",
    "#         str2 = ''\n",
    "#         switch = False\n",
    "#         for char in word:\n",
    "#             if char.isalpha() or char==\"`\" or char==\"-\":\n",
    "#                 if switch:\n",
    "#                     str2 = str2 + char\n",
    "#                 else:\n",
    "#                     str1 = str1 + char\n",
    "#             else:\n",
    "#                 switch = True\n",
    "#         count = 1\n",
    "#         tempChar = ''\n",
    "#         newStr = ''\n",
    "#         for char in str1:\n",
    "#             if char == tempChar:\n",
    "#                 count += 1\n",
    "#             tempChar = char\n",
    "#             if count < 3:\n",
    "#                 newStr += char\n",
    "#             else:\n",
    "#                 newStr = newStr[:-1]\n",
    "#                 continue\n",
    "#         count = 1\n",
    "#         tempChar = ''\n",
    "#         newStr2 = ''\n",
    "#         for char in str2:\n",
    "#             if char == tempChar:\n",
    "#                 count += 1\n",
    "#             tempChar = char\n",
    "#             if count < 3:\n",
    "#                 newStr2 += char\n",
    "#             else:\n",
    "#                 newStr2 = newStr[:-1]\n",
    "#                 continue\n",
    "                \n",
    "#         if newStr != \"\" and newStr != \"`\" and newStr != \"-\":\n",
    "#             list_to_add = np.append(list_to_add, np.array(word_vectors[spell.correction(newStr.lower())]))\n",
    "#         if newStr2 != \"\" and newStr2 != \"`\" and newStr2 != \"-\":\n",
    "#             list_to_add = np.append(list_to_add, word_vectors[spell.correction(newStr2.lower())])\n",
    "#     while list_to_add.shape[0] != 3200:\n",
    "#         list_to_add = np.append(list_to_add, np.zeros(80))\n",
    "#     list_to_add = np.reshape(list_to_add, [40, 80])\n",
    "#     test_dataset[i] = list_to_add\n",
    "#     if i % 100 == 0:\n",
    "#         print(\"Addition number: %d\" % i)\n",
    "# np.save(\"test_dataset\", test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD THE TESTING DATA AND THE MODELS\n",
    "\n",
    "test_data = np.load(\"test_dataset.npy\")\n",
    "pos_neg_model = Sequential()\n",
    "pos_neg_model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "pos_neg_model.add(Dropout(0.2))\n",
    "pos_neg_model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "pos_neg_model.add(Dropout(0.2))\n",
    "pos_neg_model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "pos_neg_model.add(Dropout(0.2))\n",
    "pos_neg_model.add(Bidirectional(LSTM(20, return_sequences=True)))\n",
    "pos_neg_model.add(Flatten())\n",
    "pos_neg_model.add(Dense(40, activation='relu'))\n",
    "pos_neg_model.build(input_shape=(None, 40, 80))\n",
    "\n",
    "neutral_model = Sequential()\n",
    "neutral_model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "neutral_model.add(Dropout(0.2))\n",
    "neutral_model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "neutral_model.add(Dropout(0.2))\n",
    "neutral_model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "neutral_model.add(Dropout(0.2))\n",
    "neutral_model.add(Bidirectional(LSTM(20, return_sequences=True)))\n",
    "neutral_model.add(Flatten())\n",
    "neutral_model.add(Dense(40, activation='relu'))\n",
    "neutral_model.build(input_shape=(None, 40, 80))\n",
    "\n",
    "pos_neg_model.load_weights(\"pos_neg_model.h5\")\n",
    "neutral_model.load_weights(\"neut_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i, test in enumerate(test_data):\n",
    "    if test_sentiment_list[i] == \"positive\" or test_sentiment_list[i] == \"negative\":\n",
    "        predictions.append(pos_neg_model.predict(np.reshape(test, [1, 40, 80])))\n",
    "    else:\n",
    "        predictions.append(neutral_model.predict(np.reshape(test, [1, 40, 80])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_selected_texts = []\n",
    "# print(test_list[2007])\n",
    "# print(test_sentiment_list[2007])\n",
    "# print(predictions[2007])\n",
    "for i, sentence in enumerate(test_list):\n",
    "    string_to_add = \"\"\n",
    "    max_elem = np.amax(predictions[i][0]) # if everything is under 0.5, just add to most likely word\n",
    "    for j, word in enumerate(sentence):\n",
    "        if max_elem < 0.5:\n",
    "            if predictions[i][0][j] == max_elem:\n",
    "                string_to_add = string_to_add + word\n",
    "        if predictions[i][0][j] >= 0.5:\n",
    "            if len(string_to_add) == 0:\n",
    "                string_to_add = string_to_add + word\n",
    "            else:\n",
    "                string_to_add = string_to_add + \" \" + word\n",
    "    test_selected_texts.append(string_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data into a pandas dataframe\n",
    "submission_file_path = \"/Users/yigitatay/Desktop/SentNLP/data/sample_submission.csv\"\n",
    "df = pd.read_csv(submission_file_path)\n",
    "textIDs = df[\"textID\"]\n",
    "\n",
    "input_to_df = list(zip(textIDs, test_selected_texts))\n",
    "final_df = pd.DataFrame(input_to_df, columns=[\"textID\", \"selected_text\"])\n",
    "final_df.to_csv(\"test_results.csv\", index=False)\n",
    "\n",
    "input_with_text = list(zip(textIDs, test_list_not_tokenized, test_selected_texts, test_sentiment_list))\n",
    "with_text_df = pd.DataFrame(input_with_text, columns=['textID', 'text', 'selected_text', 'sentiment'])\n",
    "with_text_df.to_csv(\"test_results_with_text.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
