{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Flatten, Bidirectional, \\\n",
    "                            Dropout, Embedding, Conv2D, MaxPool2D, Reshape, \\\n",
    "                            TimeDistributed, Activation, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "# TO RUN ON GPU, UNCOMMENT\n",
    "# import tensorflow as tf\n",
    "# config = tf.compat.v1.ConfigProto(device_count = {'GPU':2})\n",
    "# sess = tf.compat.v1.Session(config=config)\n",
    "# tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data into a pandas dataframe\n",
    "train_file_path = \"data/train.csv\"\n",
    "df = pd.read_csv(train_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate labels and data\n",
    "texts = df[\"text\"]\n",
    "selected_texts = df[\"selected_text\"]\n",
    "sentiments = df[\"sentiment\"]\n",
    "\n",
    "# a list to hold text, sentiment dictionaries\n",
    "train_list = []\n",
    "# a list to hold the labels\n",
    "label_list = []\n",
    "\n",
    "for text, data, label in zip(texts, sentiments, selected_texts):\n",
    "    dict_to_add = dict()\n",
    "    dict_to_add[text] = data\n",
    "    train_list.append(dict_to_add)\n",
    "    label_list.append(label)\n",
    "    \n",
    "# a list to hold specific text and label lists \n",
    "positive_train_list = []\n",
    "negative_train_list = []\n",
    "neutral_train_list = []\n",
    "# a list to hold the labels\n",
    "positive_label_list = []\n",
    "negative_label_list = []\n",
    "neutral_label_list = []\n",
    "\n",
    "i = 0\n",
    "for text, data, label in zip(texts, sentiments, selected_texts):\n",
    "    if data == \"positive\":\n",
    "        positive_train_list.append(text)\n",
    "        positive_label_list.append(label)\n",
    "    elif data == \"negative\":\n",
    "        negative_train_list.append(text)\n",
    "        negative_label_list.append(label)\n",
    "    else:\n",
    "        neutral_train_list.append(text)\n",
    "        neutral_label_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get strings with all text, positive, neutral and negative test\n",
    "# for non-deep-learning analysis\n",
    "all_text = \"\"\n",
    "all_selected_text = \"\"\n",
    "positive_text = \"\"\n",
    "positive_selected_text = \"\"\n",
    "negative_text = \"\"\n",
    "negative_selected_text = \"\"\n",
    "neutral_text = \"\"\n",
    "neutral_selected_text = \"\"\n",
    "for item, label in zip(train_list, label_list):\n",
    "    all_selected_text += (\" \" + str(label))\n",
    "    for key in item:\n",
    "        all_text += (\" \" + str(key))\n",
    "        if(item[key] == \"positive\"):\n",
    "            positive_text += (\" \" + str(key))\n",
    "            positive_selected_text += (\" \" + str(label))\n",
    "        elif(item[key] == \"negative\"):\n",
    "            negative_text += (\" \" + str(key))\n",
    "            negative_selected_text += (\" \" + str(label))\n",
    "        else:\n",
    "            neutral_text += (\" \" + str(key))\n",
    "            neutral_selected_text += (\" \" + str(label))\n",
    "\n",
    "\n",
    "chars = sorted(list(set(all_text)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set maximum tweet size to seqlen characters (dataset seems to have max about 140 chars)\n",
    "# so we pad with spaces from the beginning for each tweet\n",
    "seqlen = 160\n",
    "modified_positive_train_list = []\n",
    "i = 0\n",
    "for tweet in positive_train_list:\n",
    "    num_spaces = seqlen - len(str(tweet))\n",
    "    addition = \"\"\n",
    "    for i in range(num_spaces):\n",
    "        addition += \" \"\n",
    "    new_tweet = addition + str(tweet)\n",
    "    modified_positive_train_list.append(new_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_positive_label_list = []\n",
    "i = 0\n",
    "for selected, original in zip(positive_label_list, modified_positive_train_list):\n",
    "    selected = str(selected)\n",
    "    modified_label = []\n",
    "    whitespace_count = 0\n",
    "    j = 0\n",
    "    selected_len = len(selected)\n",
    "    while original[j:(j+selected_len)] != selected:\n",
    "        whitespace_count += 1\n",
    "        j += 1\n",
    "    for k in range(whitespace_count):\n",
    "        modified_label.append(0)\n",
    "    for k in range(selected_len):\n",
    "        modified_label.append(1)\n",
    "    while len(modified_label) != 160:\n",
    "        modified_label.append(0)\n",
    "    modified_positive_label_list.append(modified_label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add one extra dimension to the end\n",
    "x_positive = np.zeros((len(modified_positive_train_list), seqlen, len(chars)+1), dtype=np.float32)\n",
    "# y_positive = np.zeros((len(modified_positive_label_list), seqlen, len(chars)+1), dtype=np.float32)\n",
    "for i, tweet in enumerate(modified_positive_train_list):\n",
    "    for t, char in enumerate(tweet):\n",
    "        x_positive[i, t, char_indices[char]] = 1\n",
    "# for i, tweet in enumerate(modified_positive_label_list):\n",
    "#     for t, char in enumerate(tweet):\n",
    "#         y_positive[i, t, char_indices[char]] = 1 \n",
    "\n",
    "y_positive = modified_positive_label_list\n",
    "y_positive = np.asarray(modified_positive_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8582, 160)\n",
      "(8582, 160, 102, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_positive.shape)\n",
    "x_positive = np.reshape(x_positive, [8582, 160, 102, 1])\n",
    "print(x_positive.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 80, 51, 8)         80        \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 80, 51, 8)         32        \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 40, 26, 16)        1168      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 40, 26, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 20, 13, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 20, 13, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 10, 7, 16)         4624      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 10, 7, 16)         64        \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 5, 4, 8)           1160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 5, 4, 8)           32        \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 160)               25760     \n",
      "=================================================================\n",
      "Total params: 37,752\n",
      "Trainable params: 37,592\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Train on 7723 samples, validate on 859 samples\n",
      "Epoch 1/50\n",
      "7723/7723 [==============================] - 45s 6ms/sample - loss: 0.2666 - binary_crossentropy: 0.2666 - accuracy: 0.8841 - val_loss: 0.2524 - val_binary_crossentropy: 0.2524 - val_accuracy: 0.8862\n",
      "Epoch 2/50\n",
      "7723/7723 [==============================] - 45s 6ms/sample - loss: 0.2479 - binary_crossentropy: 0.2479 - accuracy: 0.8879 - val_loss: 0.2490 - val_binary_crossentropy: 0.2490 - val_accuracy: 0.8859\n",
      "Epoch 3/50\n",
      "7723/7723 [==============================] - 48s 6ms/sample - loss: 0.2421 - binary_crossentropy: 0.2421 - accuracy: 0.8886 - val_loss: 0.2523 - val_binary_crossentropy: 0.2523 - val_accuracy: 0.8815\n",
      "Epoch 4/50\n",
      "7723/7723 [==============================] - 44s 6ms/sample - loss: 0.2363 - binary_crossentropy: 0.2363 - accuracy: 0.8895 - val_loss: 0.2491 - val_binary_crossentropy: 0.2491 - val_accuracy: 0.8877\n",
      "Epoch 5/50\n",
      "7723/7723 [==============================] - 46s 6ms/sample - loss: 0.2303 - binary_crossentropy: 0.2303 - accuracy: 0.8915 - val_loss: 0.2581 - val_binary_crossentropy: 0.2581 - val_accuracy: 0.8737\n",
      "Epoch 6/50\n",
      "7723/7723 [==============================] - 46s 6ms/sample - loss: 0.2229 - binary_crossentropy: 0.2229 - accuracy: 0.8940 - val_loss: 0.2577 - val_binary_crossentropy: 0.2577 - val_accuracy: 0.8861\n",
      "Epoch 7/50\n",
      "7723/7723 [==============================] - 46s 6ms/sample - loss: 0.2149 - binary_crossentropy: 0.2149 - accuracy: 0.8979 - val_loss: 0.2665 - val_binary_crossentropy: 0.2665 - val_accuracy: 0.8824\n",
      "Epoch 8/50\n",
      "7723/7723 [==============================] - 46s 6ms/sample - loss: 0.2077 - binary_crossentropy: 0.2077 - accuracy: 0.9021 - val_loss: 0.2668 - val_binary_crossentropy: 0.2668 - val_accuracy: 0.8679\n",
      "Epoch 9/50\n",
      "7723/7723 [==============================] - 45s 6ms/sample - loss: 0.2004 - binary_crossentropy: 0.2004 - accuracy: 0.9062 - val_loss: 0.2690 - val_binary_crossentropy: 0.2690 - val_accuracy: 0.8754\n",
      "Epoch 10/50\n",
      "7723/7723 [==============================] - 45s 6ms/sample - loss: 0.1949 - binary_crossentropy: 0.1949 - accuracy: 0.9096 - val_loss: 0.2759 - val_binary_crossentropy: 0.2759 - val_accuracy: 0.8752\n",
      "Epoch 11/50\n",
      "7723/7723 [==============================] - 45s 6ms/sample - loss: 0.1883 - binary_crossentropy: 0.1883 - accuracy: 0.9142 - val_loss: 0.2865 - val_binary_crossentropy: 0.2865 - val_accuracy: 0.8669\n",
      "Epoch 12/50\n",
      "7723/7723 [==============================] - 46s 6ms/sample - loss: 0.1845 - binary_crossentropy: 0.1845 - accuracy: 0.9165 - val_loss: 0.2904 - val_binary_crossentropy: 0.2904 - val_accuracy: 0.8709\n",
      "Epoch 13/50\n",
      "7723/7723 [==============================] - 45s 6ms/sample - loss: 0.1785 - binary_crossentropy: 0.1785 - accuracy: 0.9201 - val_loss: 0.2917 - val_binary_crossentropy: 0.2917 - val_accuracy: 0.8681\n",
      "Epoch 14/50\n",
      "7723/7723 [==============================] - 46s 6ms/sample - loss: 0.1754 - binary_crossentropy: 0.1754 - accuracy: 0.9217 - val_loss: 0.2820 - val_binary_crossentropy: 0.2820 - val_accuracy: 0.8640\n",
      "Epoch 15/50\n",
      "7723/7723 [==============================] - 46s 6ms/sample - loss: 0.1712 - binary_crossentropy: 0.1712 - accuracy: 0.9242 - val_loss: 0.2950 - val_binary_crossentropy: 0.2950 - val_accuracy: 0.8664\n",
      "Epoch 16/50\n",
      "7723/7723 [==============================] - 49s 6ms/sample - loss: 0.1678 - binary_crossentropy: 0.1678 - accuracy: 0.9261 - val_loss: 0.3102 - val_binary_crossentropy: 0.3102 - val_accuracy: 0.8647\n",
      "Epoch 17/50\n",
      "7723/7723 [==============================] - 46s 6ms/sample - loss: 0.1665 - binary_crossentropy: 0.1665 - accuracy: 0.9257 - val_loss: 0.3074 - val_binary_crossentropy: 0.3074 - val_accuracy: 0.8651\n",
      "Epoch 18/50\n",
      "7723/7723 [==============================] - 46s 6ms/sample - loss: 0.1626 - binary_crossentropy: 0.1626 - accuracy: 0.9281 - val_loss: 0.3100 - val_binary_crossentropy: 0.3100 - val_accuracy: 0.8683\n",
      "Epoch 19/50\n",
      "7723/7723 [==============================] - 45s 6ms/sample - loss: 0.1596 - binary_crossentropy: 0.1596 - accuracy: 0.9301 - val_loss: 0.3097 - val_binary_crossentropy: 0.3097 - val_accuracy: 0.8601\n",
      "Epoch 20/50\n",
      "7723/7723 [==============================] - 46s 6ms/sample - loss: 0.1579 - binary_crossentropy: 0.1579 - accuracy: 0.9308 - val_loss: 0.3262 - val_binary_crossentropy: 0.3262 - val_accuracy: 0.8542\n",
      "Epoch 21/50\n",
      "7723/7723 [==============================] - 50s 6ms/sample - loss: 0.1553 - binary_crossentropy: 0.1553 - accuracy: 0.9324 - val_loss: 0.3310 - val_binary_crossentropy: 0.3310 - val_accuracy: 0.8612\n",
      "Epoch 22/50\n",
      "7723/7723 [==============================] - 46s 6ms/sample - loss: 0.1539 - binary_crossentropy: 0.1539 - accuracy: 0.9333 - val_loss: 0.3262 - val_binary_crossentropy: 0.3262 - val_accuracy: 0.8588\n",
      "Epoch 23/50\n",
      "7723/7723 [==============================] - 49s 6ms/sample - loss: 0.1516 - binary_crossentropy: 0.1516 - accuracy: 0.9340 - val_loss: 0.3202 - val_binary_crossentropy: 0.3202 - val_accuracy: 0.8695\n",
      "Epoch 24/50\n",
      "7723/7723 [==============================] - 50s 6ms/sample - loss: 0.1498 - binary_crossentropy: 0.1498 - accuracy: 0.9348 - val_loss: 0.3259 - val_binary_crossentropy: 0.3259 - val_accuracy: 0.8671\n",
      "Epoch 25/50\n",
      "7723/7723 [==============================] - 43s 6ms/sample - loss: 0.1488 - binary_crossentropy: 0.1488 - accuracy: 0.9359 - val_loss: 0.3432 - val_binary_crossentropy: 0.3432 - val_accuracy: 0.8594\n",
      "Epoch 26/50\n",
      "7723/7723 [==============================] - 47s 6ms/sample - loss: 0.1464 - binary_crossentropy: 0.1464 - accuracy: 0.9369 - val_loss: 0.3412 - val_binary_crossentropy: 0.3412 - val_accuracy: 0.8657\n",
      "Epoch 27/50\n",
      "7723/7723 [==============================] - 37s 5ms/sample - loss: 0.1447 - binary_crossentropy: 0.1447 - accuracy: 0.9375 - val_loss: 0.3311 - val_binary_crossentropy: 0.3311 - val_accuracy: 0.8662\n",
      "Epoch 28/50\n",
      "7723/7723 [==============================] - 40s 5ms/sample - loss: 0.1430 - binary_crossentropy: 0.1430 - accuracy: 0.9384 - val_loss: 0.3517 - val_binary_crossentropy: 0.3517 - val_accuracy: 0.8573\n",
      "Epoch 29/50\n",
      "7723/7723 [==============================] - 37s 5ms/sample - loss: 0.1418 - binary_crossentropy: 0.1418 - accuracy: 0.9389 - val_loss: 0.3455 - val_binary_crossentropy: 0.3455 - val_accuracy: 0.8574\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7723/7723 [==============================] - 40s 5ms/sample - loss: 0.1406 - binary_crossentropy: 0.1406 - accuracy: 0.9391 - val_loss: 0.3569 - val_binary_crossentropy: 0.3569 - val_accuracy: 0.8559\n",
      "Epoch 31/50\n",
      "7723/7723 [==============================] - 37s 5ms/sample - loss: 0.1391 - binary_crossentropy: 0.1391 - accuracy: 0.9406 - val_loss: 0.3530 - val_binary_crossentropy: 0.3530 - val_accuracy: 0.8599\n",
      "Epoch 32/50\n",
      "7723/7723 [==============================] - 40s 5ms/sample - loss: 0.1382 - binary_crossentropy: 0.1382 - accuracy: 0.9413 - val_loss: 0.3455 - val_binary_crossentropy: 0.3455 - val_accuracy: 0.8628\n",
      "Epoch 33/50\n",
      "7723/7723 [==============================] - 40s 5ms/sample - loss: 0.1366 - binary_crossentropy: 0.1366 - accuracy: 0.9418 - val_loss: 0.3529 - val_binary_crossentropy: 0.3529 - val_accuracy: 0.8597\n",
      "Epoch 34/50\n",
      "7723/7723 [==============================] - 40s 5ms/sample - loss: 0.1359 - binary_crossentropy: 0.1359 - accuracy: 0.9418 - val_loss: 0.3763 - val_binary_crossentropy: 0.3763 - val_accuracy: 0.8534\n",
      "Epoch 35/50\n",
      "7723/7723 [==============================] - 40s 5ms/sample - loss: 0.1350 - binary_crossentropy: 0.1350 - accuracy: 0.9423 - val_loss: 0.3473 - val_binary_crossentropy: 0.3473 - val_accuracy: 0.8597\n",
      "Epoch 36/50\n",
      "7723/7723 [==============================] - 39s 5ms/sample - loss: 0.1339 - binary_crossentropy: 0.1339 - accuracy: 0.9426 - val_loss: 0.3463 - val_binary_crossentropy: 0.3463 - val_accuracy: 0.8627\n",
      "Epoch 37/50\n",
      "7723/7723 [==============================] - 35s 5ms/sample - loss: 0.1330 - binary_crossentropy: 0.1330 - accuracy: 0.9435 - val_loss: 0.3469 - val_binary_crossentropy: 0.3469 - val_accuracy: 0.8566\n",
      "Epoch 38/50\n",
      "7723/7723 [==============================] - 43s 6ms/sample - loss: 0.1302 - binary_crossentropy: 0.1302 - accuracy: 0.9447 - val_loss: 0.3581 - val_binary_crossentropy: 0.3581 - val_accuracy: 0.8562\n",
      "Epoch 39/50\n",
      "7723/7723 [==============================] - 39s 5ms/sample - loss: 0.1314 - binary_crossentropy: 0.1314 - accuracy: 0.9437 - val_loss: 0.3781 - val_binary_crossentropy: 0.3781 - val_accuracy: 0.8499\n",
      "Epoch 40/50\n",
      "7723/7723 [==============================] - 38s 5ms/sample - loss: 0.1301 - binary_crossentropy: 0.1301 - accuracy: 0.9448 - val_loss: 0.3828 - val_binary_crossentropy: 0.3828 - val_accuracy: 0.8585\n",
      "Epoch 41/50\n",
      "7723/7723 [==============================] - 39s 5ms/sample - loss: 0.1291 - binary_crossentropy: 0.1291 - accuracy: 0.9448 - val_loss: 0.3683 - val_binary_crossentropy: 0.3683 - val_accuracy: 0.8564\n",
      "Epoch 42/50\n",
      "7723/7723 [==============================] - 38s 5ms/sample - loss: 0.1280 - binary_crossentropy: 0.1280 - accuracy: 0.9454 - val_loss: 0.3976 - val_binary_crossentropy: 0.3976 - val_accuracy: 0.8512\n",
      "Epoch 43/50\n",
      "7723/7723 [==============================] - 41s 5ms/sample - loss: 0.1275 - binary_crossentropy: 0.1275 - accuracy: 0.9453 - val_loss: 0.3814 - val_binary_crossentropy: 0.3814 - val_accuracy: 0.8579\n",
      "Epoch 44/50\n",
      "7723/7723 [==============================] - 40s 5ms/sample - loss: 0.1279 - binary_crossentropy: 0.1279 - accuracy: 0.9458 - val_loss: 0.3915 - val_binary_crossentropy: 0.3915 - val_accuracy: 0.8635\n",
      "Epoch 45/50\n",
      "7723/7723 [==============================] - 43s 6ms/sample - loss: 0.1266 - binary_crossentropy: 0.1266 - accuracy: 0.9463 - val_loss: 0.3828 - val_binary_crossentropy: 0.3828 - val_accuracy: 0.8559\n",
      "Epoch 46/50\n",
      "7723/7723 [==============================] - 39s 5ms/sample - loss: 0.1241 - binary_crossentropy: 0.1241 - accuracy: 0.9475 - val_loss: 0.4045 - val_binary_crossentropy: 0.4045 - val_accuracy: 0.8581\n",
      "Epoch 47/50\n",
      "7723/7723 [==============================] - 37s 5ms/sample - loss: 0.1235 - binary_crossentropy: 0.1235 - accuracy: 0.9477 - val_loss: 0.3898 - val_binary_crossentropy: 0.3898 - val_accuracy: 0.8581\n",
      "Epoch 48/50\n",
      "7723/7723 [==============================] - 40s 5ms/sample - loss: 0.1234 - binary_crossentropy: 0.1234 - accuracy: 0.9476 - val_loss: 0.3947 - val_binary_crossentropy: 0.3947 - val_accuracy: 0.8603\n",
      "Epoch 49/50\n",
      "7723/7723 [==============================] - 35s 5ms/sample - loss: 0.1240 - binary_crossentropy: 0.1240 - accuracy: 0.9473 - val_loss: 0.3855 - val_binary_crossentropy: 0.3855 - val_accuracy: 0.8594\n",
      "Epoch 50/50\n",
      "7723/7723 [==============================] - 39s 5ms/sample - loss: 0.1226 - binary_crossentropy: 0.1226 - accuracy: 0.9478 - val_loss: 0.4093 - val_binary_crossentropy: 0.4093 - val_accuracy: 0.8611\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(160, 102, 1), name='model_input'))\n",
    "model.add(Conv2D(filters=8, kernel_size=3, strides=2, padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=16, kernel_size=3, strides=2, padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=32, kernel_size=3, strides=2, padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=16, kernel_size=3, strides=2, padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=8, kernel_size=3, strides=2, padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(160, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.build(input_shape=(None, 160, 102))\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=['binary_crossentropy', 'accuracy'])\n",
    "\n",
    "model.fit(x_positive, y_positive,\n",
    "          batch_size=2,\n",
    "          epochs=50, \n",
    "          validation_split=0.1)\n",
    "model.save(\"three_cnn_positive_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data into a pandas dataframe\n",
    "test_file_path = \"data/test.csv\"\n",
    "df_test = pd.read_csv(test_file_path)\n",
    "# separate labels and data\n",
    "texts = df_test[\"text\"]\n",
    "sentiments = df_test[\"sentiment\"]\n",
    "\n",
    "# a list to hold text, sentiment dictionaries\n",
    "test_list = []\n",
    "\n",
    "for text, data in zip(texts, sentiments):\n",
    "    dict_to_add = dict()\n",
    "    dict_to_add[text] = data\n",
    "    test_list.append(dict_to_add)\n",
    "    \n",
    "# a list to hold specific text and label lists \n",
    "positive_test_list = []\n",
    "negative_test_list = []\n",
    "neutral_test_list = []\n",
    "\n",
    "i = 0\n",
    "for text, data in zip(texts, sentiments):\n",
    "    if data == \"positive\":\n",
    "        positive_test_list.append(text)\n",
    "    elif data == \"negative\":\n",
    "        negative_test_list.append(text)\n",
    "    else:\n",
    "        neutral_test_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get strings with all text, positive, neutral and negative test\n",
    "all_text = \"\"\n",
    "positive_text = \"\"\n",
    "negative_text = \"\"\n",
    "neutral_text = \"\"\n",
    "for item in zip(test_list):\n",
    "    all_text += (\" \" + str(item))\n",
    "    if(item == \"positive\"):\n",
    "        positive_text += (\" \" + str(item))\n",
    "    elif(item == \"negative\"):\n",
    "        negative_text += (\" \" + str(item))\n",
    "    else:\n",
    "        neutral_text += (\" \" + str(item))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set maximum tweet size to seqlen characters (dataset seems to have max about 140 chars)\n",
    "# so we pad with spaces from the beginning for each tweet\n",
    "seqlen = 160\n",
    "modified_positive_test_list = []\n",
    "i = 0\n",
    "for tweet in positive_test_list:\n",
    "    num_spaces = seqlen - len(str(tweet))\n",
    "    addition = \"\"\n",
    "    for i in range(num_spaces):\n",
    "        addition += \" \"\n",
    "    new_tweet = addition + str(tweet)\n",
    "    modified_positive_test_list.append(new_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add one extra dimension to the end\n",
    "x_positive_test = np.zeros((len(modified_positive_test_list), seqlen, len(chars)+1), dtype=np.float32)\n",
    "# y_positive = np.zeros((len(modified_positive_label_list), seqlen, len(chars)+1), dtype=np.float32)\n",
    "for i, tweet in enumerate(modified_positive_test_list):\n",
    "    for t, char in enumerate(tweet):\n",
    "        try:\n",
    "            x_positive_test[i, t, char_indices[char]] = 1\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_train = model.predict(np.reshape(x_positive[600], [1, 160, 102, 1]))\n",
    "result_test1 = model.predict(np.reshape(x_positive_test[20], [1, 160, 102, 1]))\n",
    "result_test2 = model.predict(np.reshape(x_positive_test[21], [1, 160, 102, 1]))\n",
    "result_test3 = model.predict(np.reshape(x_positive_test[22], [1, 160, 102, 1]))\n",
    "result_test4 = model.predict(np.reshape(x_positive_test[23], [1, 160, 102, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for training data:\n",
      "\n",
      "                                                                                                                    Had a good time at Flap-a-taco with , , and \n",
      "a \n",
      "good\n",
      "\n",
      "Test for testing data:\n",
      "\n",
      "                                                                       Yay for Block Party!  You`re the BOMB! Blockheads <3 Dave!   Thanks for supporting NKOTB!\n",
      "k\n",
      "\n",
      "                                                                           Happy Birthday Snickers!!!! ? I hope you have the best day ever! Let`s go shopping!!!\n",
      "phat\n",
      "\n",
      "                                                                                                                                   Thank you!  I`m working on `s\n",
      "   Thank you!  I`m working on `s\n",
      "\n",
      "                                                                                                                                            Happy Mothers Day!!!\n",
      "  Happy Mothers Day!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Test for training data:\\n\")\n",
    "train_str = \"\"\n",
    "for i, line in enumerate(modified_positive_train_list):\n",
    "    if i == 600:\n",
    "        train_str = line\n",
    "        break\n",
    "print(train_str)\n",
    "train_label_str = \"\"\n",
    "for i in range(len(train_str)):\n",
    "    if result_train[0][i] >= 0.5:\n",
    "        train_label_str = train_label_str + train_str[i]\n",
    "print(train_label_str)\n",
    "print(positive_label_list[600])\n",
    "\n",
    "print(\"\\nTest for testing data:\\n\")\n",
    "test_str = \"\"\n",
    "for i, line in enumerate(modified_positive_test_list):\n",
    "    if i == 20:\n",
    "        test_str = line\n",
    "        break\n",
    "print(test_str)\n",
    "test_label_str = \"\"\n",
    "for i in range(len(test_str)):\n",
    "    if result_test1[0][i] >= 0.1:\n",
    "        test_label_str = test_label_str + test_str[i]\n",
    "print(test_label_str + \"\\n\")\n",
    "\n",
    "test_str = \"\"\n",
    "for i, line in enumerate(modified_positive_test_list):\n",
    "    if i == 21:\n",
    "        test_str = line\n",
    "        break\n",
    "print(test_str)\n",
    "test_label_str = \"\"\n",
    "for i in range(len(test_str)):\n",
    "    if result_test2[0][i] >= 0.1:\n",
    "        test_label_str = test_label_str + test_str[i]\n",
    "print(test_label_str + \"\\n\")\n",
    "\n",
    "test_str = \"\"\n",
    "for i, line in enumerate(modified_positive_test_list):\n",
    "    if i == 22:\n",
    "        test_str = line\n",
    "        break\n",
    "print(test_str)\n",
    "test_label_str = \"\"\n",
    "for i in range(len(test_str)):\n",
    "    if result_test3[0][i] >= 0.1:\n",
    "        test_label_str = test_label_str + test_str[i]\n",
    "print(test_label_str + \"\\n\")\n",
    "\n",
    "test_str = \"\"\n",
    "for i, line in enumerate(modified_positive_test_list):\n",
    "    if i == 23:\n",
    "        test_str = line\n",
    "        break\n",
    "print(test_str)\n",
    "test_label_str = \"\"\n",
    "for i in range(len(test_str)):\n",
    "    if result_test4[0][i] >= 0.1:\n",
    "        test_label_str = test_label_str + test_str[i]\n",
    "print(test_label_str + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
