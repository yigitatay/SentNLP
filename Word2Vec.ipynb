{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data into a pandas dataframe\n",
    "train_file_path = \"/Users/yigitatay/Desktop/SentNLP/data/train.csv\"\n",
    "df = pd.read_csv(train_file_path)\n",
    "test_file_path = \"/Users/yigitatay/Desktop/SentNLP/data/test.csv\"\n",
    "df_test = pd.read_csv(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate labels and data\n",
    "texts = df[\"text\"]\n",
    "selected_texts = df[\"selected_text\"]\n",
    "sentiments = df[\"sentiment\"]\n",
    "# a list to hold text, sentiment dictionaries\n",
    "train_list = []\n",
    "# a list to hold the labels\n",
    "label_list = []\n",
    "only_texts = []\n",
    "\n",
    "for text, data, label in zip(texts, sentiments, selected_texts):\n",
    "    dict_to_add = dict()\n",
    "    dict_to_add[text] = data\n",
    "    train_list.append(dict_to_add)\n",
    "    label_list.append(label)\n",
    "    list_to_add = str(text).split()\n",
    "    only_texts.append(list_to_add)\n",
    "    \n",
    "# separate labels and data\n",
    "texts = df_test[\"text\"]\n",
    "sentiments = df_test[\"sentiment\"]\n",
    "# a list to hold text, sentiment dictionaries\n",
    "test_list = []\n",
    "\n",
    "for text, data in zip(texts, sentiments):\n",
    "    dict_to_add = dict()\n",
    "    dict_to_add[text] = data\n",
    "    test_list.append(dict_to_add)\n",
    "    list_to_add = text.split()\n",
    "    only_texts.append(list_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get strings with all text, positive, neutral and negative test\n",
    "# for non-deep-learning analysis\n",
    "all_text_train = \"\"\n",
    "all_text_test = \"\"\n",
    "all_text = \"\"\n",
    "all_selected_text = \"\"\n",
    "positive_text = \"\"\n",
    "positive_selected_text = \"\"\n",
    "negative_text = \"\"\n",
    "negative_selected_text = \"\"\n",
    "neutral_text = \"\"\n",
    "neutral_selected_text = \"\"\n",
    "for item, label in zip(train_list, label_list):\n",
    "    all_selected_text += (\" \" + str(label))\n",
    "    for key in item:\n",
    "        all_text_train += (\" \" + str(key))\n",
    "        if(item[key] == \"positive\"):\n",
    "            positive_text += (\" \" + str(key))\n",
    "            positive_selected_text += (\" \" + str(label))\n",
    "        elif(item[key] == \"negative\"):\n",
    "            negative_text += (\" \" + str(key))\n",
    "            negative_selected_text += (\" \" + str(label))\n",
    "        else:\n",
    "            neutral_text += (\" \" + str(key))\n",
    "            neutral_selected_text += (\" \" + str(label))\n",
    "            \n",
    "for item in test_list:\n",
    "    for key in item:\n",
    "        all_text_test += (\" \" + str(key))\n",
    "        if(item[key] == \"positive\"):\n",
    "            positive_text += (\" \" + str(key))\n",
    "            positive_selected_text += (\" \" + str(label))\n",
    "        elif(item[key] == \"negative\"):\n",
    "            negative_text += (\" \" + str(key))\n",
    "            negative_selected_text += (\" \" + str(label))\n",
    "        else:\n",
    "            neutral_text += (\" \" + str(key))\n",
    "            neutral_selected_text += (\" \" + str(label))\n",
    "all_text = all_text_train + all_text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_list = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31015\n"
     ]
    }
   ],
   "source": [
    "spell = SpellChecker()\n",
    "word2vec_train = []\n",
    "add_this_list = []\n",
    "print(len(only_texts))\n",
    "for i, lst in enumerate(only_texts):\n",
    "    for word in lst:\n",
    "        if word[0:4] == \"http\":\n",
    "            continue\n",
    "        if \"****\" in word:\n",
    "            word = word.replace('****', 'censored')\n",
    "        str1 = ''\n",
    "        str2 = ''\n",
    "        switch = False\n",
    "        for char in word:\n",
    "            if char.isalpha() or char==\"`\" or char==\"-\":\n",
    "                if switch:\n",
    "                    str2 = str2 + char\n",
    "                else:\n",
    "                    str1 = str1 + char\n",
    "            else:\n",
    "                switch = True\n",
    "        count = 1\n",
    "        tempChar = ''\n",
    "        newStr = ''\n",
    "        for char in str1:\n",
    "            if char == tempChar:\n",
    "                count += 1\n",
    "            tempChar = char\n",
    "            if count < 3:\n",
    "                newStr += char\n",
    "            else:\n",
    "                newStr = newStr[:-1]\n",
    "                continue\n",
    "        count = 1\n",
    "        tempChar = ''\n",
    "        newStr2 = ''\n",
    "        for char in str2:\n",
    "            if char == tempChar:\n",
    "                count += 1\n",
    "            tempChar = char\n",
    "            if count < 3:\n",
    "                newStr2 += char\n",
    "            else:\n",
    "                newStr2 = newStr[:-1]\n",
    "                continue\n",
    "        if newStr != \"\" and newStr != \"`\" and newStr != \"-\":\n",
    "#             add_this_list.append(spell.correction(newStr.lower()))\n",
    "            add_this_list.append(newStr.lower())\n",
    "        if newStr2 != \"\" and newStr2 != \"`\" and newStr2 != \"-\":\n",
    "#             add_this_list.append(spell.correction(newStr2.lower()))\n",
    "            add_this_list.append(newStr2.lower())\n",
    "    word2vec_train.append(add_this_list)\n",
    "    add_this_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(common_texts, size=40, window=5, min_count=1, workers=4)\n",
    "model.train(word2vec_train, total_examples=len(word2vec_train), epochs=30)\n",
    "model.save(\"word2vec_20.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'bad' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-af35c7ad51c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bad\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"france\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'bad' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "word_vectors = model.wv\n",
    "print(word_vectors[\"bad\"])\n",
    "word_vectors.most_similar(positive=\"france\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Word2Vec.load(\"word2vec_checkpoints/word2vec_80.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.7999861e+00  3.4378700e+00  1.3343029e-01 -1.7258933e+00\n",
      " -2.1933072e+00 -1.2163146e+00 -4.2054425e-03 -4.7087115e-01\n",
      " -2.8000445e+00 -3.0226305e+00 -1.1610838e+00  3.0894806e+00\n",
      "  1.4331795e-01 -3.2117149e-01 -3.2210478e-01 -4.9234846e-01\n",
      " -1.2134603e+00  2.8057344e+00  1.3026572e+00 -3.3423722e-01\n",
      "  2.5895295e+00 -3.6200345e-01 -3.0259647e+00  1.0741885e+00\n",
      "  3.7096077e-01  3.8122158e+00 -3.8140813e-01 -5.0311503e+00\n",
      "  1.2573861e+00  1.7053988e-02 -4.8959839e-01  8.0294943e-01\n",
      "  6.2059212e-01  1.2024442e+00  1.9188854e+00  1.6187038e+00\n",
      "  4.4396541e-01 -1.7715130e+00  1.4294227e+00  3.8013194e+00\n",
      " -1.3466917e+00 -1.5571499e+00  2.0741105e+00 -2.5135386e-01\n",
      "  6.2788832e-01  2.3422976e+00 -3.5756688e+00  2.7249253e+00\n",
      "  1.3126025e+00  6.3044471e-01  1.0292249e+00  5.0902641e-01\n",
      " -2.8560450e+00 -2.5765204e+00 -2.1603413e+00 -1.6816295e+00\n",
      "  4.6173251e-01  1.5901607e-01 -8.7663877e-01  2.0076752e+00\n",
      "  8.6416072e-01  5.6010729e-01  6.6222802e-02 -8.6823165e-01\n",
      "  2.3556492e+00 -1.2396886e+00 -1.9107838e+00  2.7972591e+00\n",
      "  3.7383246e-01 -2.0321272e-01  2.1201961e-01 -2.8128123e-01\n",
      " -6.9292176e-01 -1.5625237e-01  4.0493235e-01  8.5521877e-01\n",
      " -1.1179811e+00 -1.9982599e+00 -6.8137240e-01 -1.4370619e+00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('dallas', 0.6668902635574341),\n",
       " ('london', 0.5933231115341187),\n",
       " ('paso', 0.5720329284667969),\n",
       " ('vegas', 0.5719572305679321),\n",
       " ('francisco', 0.5640627145767212),\n",
       " ('california', 0.5627366304397583),\n",
       " ('tangerine', 0.5569376945495605),\n",
       " ('deg', 0.5531080961227417),\n",
       " ('delay', 0.5504189729690552),\n",
       " ('garden', 0.5501287579536438)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vecs = new_model.wv\n",
    "print(word_vecs[\"bad\"])\n",
    "word_vecs.most_similar(positive=\"france\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
