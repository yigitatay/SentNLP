{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Flatten, Bidirectional, \\\n",
    "                            Dropout, Embedding, Conv2D, MaxPool2D, Reshape, \\\n",
    "                            TimeDistributed, Activation, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "# TO RUN ON GPU, UNCOMMENT\n",
    "# import tensorflow as tf\n",
    "# config = tf.compat.v1.ConfigProto(device_count = {'GPU':2})\n",
    "# sess = tf.compat.v1.Session(config=config)\n",
    "# tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data into a pandas dataframe\n",
    "train_file_path = \"/Users/yigitatay/Desktop/SentNLP/data/train.csv\"\n",
    "df = pd.read_csv(train_file_path)\n",
    "test_file_path = \"/Users/yigitatay/Desktop/SentNLP/data/test.csv\"\n",
    "df_test = pd.read_csv(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate labels and data\n",
    "texts = df[\"text\"]\n",
    "selected_texts = df[\"selected_text\"]\n",
    "sentiments = df[\"sentiment\"]\n",
    "# lists to hold text, sentiment\n",
    "train_list = []\n",
    "train_sentiment_list = []\n",
    "# a list to hold the labels\n",
    "label_list = []\n",
    "for text, data, label in zip(texts, sentiments, selected_texts):\n",
    "    text = str(text).split()\n",
    "    train_list.append(text)\n",
    "    label = str(label).split()\n",
    "    label_list.append(label)\n",
    "    train_sentiment_list.append(data)\n",
    "    \n",
    "# separate labels and data\n",
    "texts = df_test[\"text\"]\n",
    "sentiments = df_test[\"sentiment\"]\n",
    "# lists to hold text, sentiment\n",
    "test_list = []\n",
    "test_sentiment_list = []\n",
    "for text, data in zip(texts, sentiments):\n",
    "    text = str(text).split()\n",
    "    test_list.append(text)\n",
    "    test_sentiment_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27481\n"
     ]
    }
   ],
   "source": [
    "print(len(train_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD THE WORD VECTORS FROM THE PRETRAINED WORD2VEC MODEL\n",
    "model = Word2Vec.load(\"word2vec_checkpoints/word2vec_80.model\")\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum number of words in a tweet in this dataset is 33, so 40 would be a good standard for sentence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UNCOMMENT TO RECREATE THE DATASET IF IT'S NOT SAVED\n",
    "\n",
    "# spell = SpellChecker()\n",
    "# input_dataset = np.zeros((27481, 40, 80), dtype=float)\n",
    "# for i, sentence in enumerate(train_list):\n",
    "#     list_to_add = np.array([])\n",
    "#     for word in sentence:\n",
    "#         # if it's a link, add a vector of 0's\n",
    "#         if word[0:4] == \"http\":\n",
    "#             list_to_add = np.append(list_to_add, np.zeros(80))\n",
    "#             continue\n",
    "#         if \"****\" in word:\n",
    "#             word = word.replace('****', 'censored')\n",
    "#         str1 = ''\n",
    "#         str2 = ''\n",
    "#         switch = False\n",
    "#         for char in word:\n",
    "#             if char.isalpha() or char==\"`\" or char==\"-\":\n",
    "#                 if switch:\n",
    "#                     str2 = str2 + char\n",
    "#                 else:\n",
    "#                     str1 = str1 + char\n",
    "#             else:\n",
    "#                 switch = True\n",
    "#         count = 1\n",
    "#         tempChar = ''\n",
    "#         newStr = ''\n",
    "#         for char in str1:\n",
    "#             if char == tempChar:\n",
    "#                 count += 1\n",
    "#             tempChar = char\n",
    "#             if count < 3:\n",
    "#                 newStr += char\n",
    "#             else:\n",
    "#                 newStr = newStr[:-1]\n",
    "#                 continue\n",
    "#         count = 1\n",
    "#         tempChar = ''\n",
    "#         newStr2 = ''\n",
    "#         for char in str2:\n",
    "#             if char == tempChar:\n",
    "#                 count += 1\n",
    "#             tempChar = char\n",
    "#             if count < 3:\n",
    "#                 newStr2 += char\n",
    "#             else:\n",
    "#                 newStr2 = newStr[:-1]\n",
    "#                 continue\n",
    "                \n",
    "#         if newStr != \"\" and newStr != \"`\" and newStr != \"-\":\n",
    "#             list_to_add = np.append(list_to_add, np.array(word_vectors[spell.correction(newStr.lower())]))\n",
    "#         if newStr2 != \"\" and newStr2 != \"`\" and newStr2 != \"-\":\n",
    "#             list_to_add = np.append(list_to_add, word_vectors[spell.correction(newStr2.lower())])\n",
    "#     while list_to_add.shape[0] != 3200:\n",
    "#         list_to_add = np.append(list_to_add, np.zeros(80))\n",
    "#     list_to_add = np.reshape(list_to_add, [40, 80])\n",
    "#     input_dataset[i] = list_to_add\n",
    "#     if i % 100 == 0:\n",
    "#         print(\"Addition number: %d\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(input_dataset))\n",
    "# input_array = []\n",
    "# input_add = []\n",
    "# for array in input_dataset:\n",
    "#     for item in array:\n",
    "#         input_add.append(item.tolist())\n",
    "#     input_array.append(input_add)\n",
    "# input_array = np.asarray(input_array)\n",
    "# np.save(\"input_without_sentiment\", input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive tweets: 8582\n",
      "Negative tweets: 7781\n",
      "Neutral tweets: 19700\n",
      "Total non-neutral tweets: 16363\n"
     ]
    }
   ],
   "source": [
    "count_pos = 0\n",
    "count_neg = 0\n",
    "count_neut = 0\n",
    "for sent in train_sentiment_list:\n",
    "    if sent == \"positive\":\n",
    "        count_pos += 1\n",
    "    if sent == \"negative\":\n",
    "        count_neg += 1\n",
    "    else:\n",
    "        count_neut += 1\n",
    "print(\"Positive tweets: \" + str(count_pos))\n",
    "print(\"Negative tweets: \" + str(count_neg))\n",
    "print(\"Neutral tweets: \" + str(count_neut))\n",
    "print(\"Total non-neutral tweets: \" + str(count_pos + count_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO LOAD THE DATASET (WITHOUT SENTIMENT)\n",
    "train_array = np.load(\"input_without_sentiment.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UNCOMMENT TO CREATE THE DATASET WITH THE SENTIMENTS\n",
    "# Concatanate the sentiment on each word\n",
    "# # pos_array = np.ones((40, 80), dtype=float)\n",
    "# # neg_array = -np.ones((40, 80), dtype=float)\n",
    "# # neutral_array = np.zeros((40, 80), dtype=float)\n",
    "\n",
    "# train_array_with_sent = np.zeros((train_array.shape[0], 40, 80, 2))\n",
    "# pos_neg_array = \n",
    "# for i in range(train_array.shape[0]):\n",
    "#     if train_sentiment_list[i] == \"positive\":\n",
    "#         result = np.dstack((train_array[i], neutral_array))\n",
    "#     elif train_sentiment_list[i] == \"negative\":\n",
    "#         result = np.dstack((train_array[i], neutral_array))\n",
    "#     else:\n",
    "#         result = np.dstack((train_array[i], neutral_array))\n",
    "#     train_array_with_sent[i] = result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19700, 40, 80)\n",
      "(16363, 40, 80)\n"
     ]
    }
   ],
   "source": [
    "pos_neg_array = np.zeros((count_pos+count_neg, 40, 80))\n",
    "neutral_array = np.zeros((count_neut, 40, 80))\n",
    "pos_neg_index = 0\n",
    "neut_index = 0\n",
    "for i in range(train_array.shape[0]):\n",
    "    if train_sentiment_list[i] == \"positive\" or train_sentiment_list[i] == \"negative\":\n",
    "        pos_neg_array[pos_neg_index] = train_array[i]\n",
    "        pos_neg_index += 1\n",
    "    else:\n",
    "        neutral_array[neut_index] = train_array[i]\n",
    "        neut_index += 1\n",
    "print(neutral_array.shape)\n",
    "print(pos_neg_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"input_with_sentiment\", train_array_with_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UNCOMMENT TO CREATE THE LABELS FOR TRAINING\n",
    "# label_train = np.zeros((len(train_list), 40), dtype=np.float32)\n",
    "# label_train.fill(.2)\n",
    "# for (num, item), label in zip(enumerate(train_list), label_list):\n",
    "#     loc = 0\n",
    "#     for i, word in enumerate(item):\n",
    "#         if loc != len(label) and (label[loc] == word[0:len(label[loc])] or label[loc] == word[-len(label[loc]):]):\n",
    "#             label_train[num][i] = .8\n",
    "#             loc += 1\n",
    "#         else:\n",
    "#             loc = 0\n",
    "# np.save(\"labels\", label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = np.zeros((len(train_list), 40), dtype=np.float32)\n",
    "label_train.fill(0.2)\n",
    "for (num, item), label in zip(enumerate(train_list), label_list):\n",
    "    loc = 0\n",
    "    for i, word in enumerate(item):\n",
    "        if loc != len(label) and (label[loc] == word[0:len(label[loc])] or label[loc] == word[-len(label[loc]):]):\n",
    "            label_train[num][i] = 0.8\n",
    "            loc += 1\n",
    "        else:\n",
    "            loc = 0\n",
    "label_pos_neg = np.zeros((count_pos+count_neg, 40), dtype=np.float32)\n",
    "label_neut = np.zeros((count_neut, 40), dtype=np.float32)\n",
    "label_pos_neg.fill(0.2)\n",
    "label_neut.fill(0.2)\n",
    "neut_index = 0\n",
    "pos_neg_index = 0\n",
    "for i in range(len(label_list)):\n",
    "    if train_sentiment_list[i] == \"positive\" or train_sentiment_list[i] == \"negative\":\n",
    "        label_pos_neg[pos_neg_index] = label_train[i]\n",
    "        pos_neg_index += 1\n",
    "    else:\n",
    "        label_neut[neut_index] = label_train[i]\n",
    "        neut_index += 1\n",
    "np.save(\"labels_pos_neg\", label_pos_neg)\n",
    "np.save(\"labels_neut\", label_neut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = np.load(\"labels.npy\")\n",
    "# #train = np.load(\"input_wit_sentiment.npy\")\n",
    "# train = np.load(\"input_without_sentiment.npy\")\n",
    "pos_neg_labels = np.load(\"labels_pos_neg.npy\")\n",
    "neut_labels = np.load(\"labels_neut.npy\")\n",
    "\n",
    "pos_neg_train = pos_neg_array\n",
    "neut_train = neutral_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_20 (Bidirectio multiple                  28928     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "bidirectional_21 (Bidirectio multiple                  24832     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio multiple                  24832     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "bidirectional_23 (Bidirectio multiple                  13600     \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  64040     \n",
      "=================================================================\n",
      "Total params: 156,232\n",
      "Trainable params: 156,232\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 14726 samples, validate on 1637 samples\n",
      "Epoch 1/50\n",
      "14726/14726 [==============================] - 522s 35ms/sample - loss: 0.5580 - binary_crossentropy: 0.5580 - accuracy: 0.0000e+00 - val_loss: 0.5527 - val_binary_crossentropy: 0.5527 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "14726/14726 [==============================] - 506s 34ms/sample - loss: 0.5500 - binary_crossentropy: 0.5500 - accuracy: 0.0000e+00 - val_loss: 0.5503 - val_binary_crossentropy: 0.5503 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "14726/14726 [==============================] - 507s 34ms/sample - loss: 0.5473 - binary_crossentropy: 0.5473 - accuracy: 0.0000e+00 - val_loss: 0.5480 - val_binary_crossentropy: 0.5480 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      " 1046/14726 [=>............................] - ETA: 8:24 - loss: 0.5483 - binary_crossentropy: 0.5483 - accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# model.add(Input(shape=(40, 80, 2), name='model_input'))\n",
    "# model.add(Conv2D(filters=8, kernel_size=3, strides=2, padding='same', activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(filters=16, kernel_size=3, strides=2, padding='same', activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(40, activation='relu'))\n",
    "\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(20, return_sequences=True)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(40, activation='relu'))\n",
    "\n",
    "model.build(input_shape=(None, 40, 80))\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=['binary_crossentropy', 'accuracy'])\n",
    "\n",
    "model.fit(pos_neg_train, pos_neg_labels,\n",
    "          batch_size=2,\n",
    "          epochs=50, \n",
    "          validation_split=0.1)\n",
    "model.save(\"pos_neg_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 80)\n"
     ]
    }
   ],
   "source": [
    "print(train[0].shape)\n",
    "result = model.predict(np.reshape(train[1125], [1, 40, 80]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30501285 0.30679423 0.31108868 0.31498742 0.30972064 0.35368824\n",
      "  0.42279556 0.54031336 0.69489515 0.54391897 0.50401616 0.24578306\n",
      "  0.18780696 0.18869683 0.19251756 0.19301684 0.19562441 0.18737917\n",
      "  0.18658817 0.19227588 0.19187032 0.18781409 0.18255234 0.18702105\n",
      "  0.18850556 0.19334635 0.19075967 0.19518043 0.19530396 0.19848514\n",
      "  0.19878377 0.19895178 0.2008979  0.20091653 0.2007593  0.20156424\n",
      "  0.2008959  0.20103452 0.20172264 0.20042361]]\n",
      "['the', 'free', 'fillin`', 'app', 'on', 'my', 'ipod', 'is', 'fun,', 'im', 'addicted']\n",
      "['the', 'free', 'fillin`', 'app', 'on', 'my', 'ipod', 'is', 'fun,', 'im', 'addicted']\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "print(result)\n",
    "print(label_list[1125])\n",
    "print(train_list[1125])\n",
    "print(train_sentiment_list[1125])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition number: 0\n",
      "Addition number: 100\n",
      "Addition number: 200\n",
      "Addition number: 300\n",
      "Addition number: 400\n",
      "Addition number: 500\n",
      "Addition number: 600\n",
      "Addition number: 700\n",
      "Addition number: 800\n",
      "Addition number: 900\n",
      "Addition number: 1000\n",
      "Addition number: 1100\n",
      "Addition number: 1200\n",
      "Addition number: 1300\n",
      "Addition number: 1400\n",
      "Addition number: 1500\n",
      "Addition number: 1600\n",
      "Addition number: 1700\n",
      "Addition number: 1800\n",
      "Addition number: 1900\n",
      "Addition number: 2000\n",
      "Addition number: 2100\n",
      "Addition number: 2200\n",
      "Addition number: 2300\n",
      "Addition number: 2400\n",
      "Addition number: 2500\n",
      "Addition number: 2600\n",
      "Addition number: 2700\n",
      "Addition number: 2800\n",
      "Addition number: 2900\n",
      "Addition number: 3000\n",
      "Addition number: 3100\n",
      "Addition number: 3200\n",
      "Addition number: 3300\n",
      "Addition number: 3400\n",
      "Addition number: 3500\n"
     ]
    }
   ],
   "source": [
    "## UNCOMMENT TO RECREATE THE TEST DATASET IF IT'S NOT SAVED\n",
    "\n",
    "spell = SpellChecker()\n",
    "test_dataset = np.zeros((len(test_list), 40, 80), dtype=float)\n",
    "for i, sentence in enumerate(test_list):\n",
    "    list_to_add = np.array([])\n",
    "    for word in sentence:\n",
    "        # if it's a link, add a vector of 0's\n",
    "        if word[0:4] == \"http\":\n",
    "            list_to_add = np.append(list_to_add, np.zeros(80))\n",
    "            continue\n",
    "        if \"****\" in word:\n",
    "            word = word.replace('****', 'censored')\n",
    "        str1 = ''\n",
    "        str2 = ''\n",
    "        switch = False\n",
    "        for char in word:\n",
    "            if char.isalpha() or char==\"`\" or char==\"-\":\n",
    "                if switch:\n",
    "                    str2 = str2 + char\n",
    "                else:\n",
    "                    str1 = str1 + char\n",
    "            else:\n",
    "                switch = True\n",
    "        count = 1\n",
    "        tempChar = ''\n",
    "        newStr = ''\n",
    "        for char in str1:\n",
    "            if char == tempChar:\n",
    "                count += 1\n",
    "            tempChar = char\n",
    "            if count < 3:\n",
    "                newStr += char\n",
    "            else:\n",
    "                newStr = newStr[:-1]\n",
    "                continue\n",
    "        count = 1\n",
    "        tempChar = ''\n",
    "        newStr2 = ''\n",
    "        for char in str2:\n",
    "            if char == tempChar:\n",
    "                count += 1\n",
    "            tempChar = char\n",
    "            if count < 3:\n",
    "                newStr2 += char\n",
    "            else:\n",
    "                newStr2 = newStr[:-1]\n",
    "                continue\n",
    "                \n",
    "        if newStr != \"\" and newStr != \"`\" and newStr != \"-\":\n",
    "            list_to_add = np.append(list_to_add, np.array(word_vectors[spell.correction(newStr.lower())]))\n",
    "        if newStr2 != \"\" and newStr2 != \"`\" and newStr2 != \"-\":\n",
    "            list_to_add = np.append(list_to_add, word_vectors[spell.correction(newStr2.lower())])\n",
    "    while list_to_add.shape[0] != 3200:\n",
    "        list_to_add = np.append(list_to_add, np.zeros(80))\n",
    "    list_to_add = np.reshape(list_to_add, [40, 80])\n",
    "    test_dataset[i] = list_to_add\n",
    "    if i % 100 == 0:\n",
    "        print(\"Addition number: %d\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 80)\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset[0].shape)\n",
    "result = model.predict(np.reshape(test_dataset[125], [1, 40, 80]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45378008 0.52638525 0.5755636  0.7686279  0.41885033 0.36972898\n",
      "  0.35467526 0.36100286 0.36500722 0.34273344 0.35559586 0.3651407\n",
      "  0.37919408 0.39935568 0.38550937 0.24221063 0.20775586 0.19622347\n",
      "  0.19004185 0.18969296 0.19104597 0.19016522 0.1878237  0.19124797\n",
      "  0.19144896 0.19519086 0.18997876 0.19505893 0.19710167 0.19690603\n",
      "  0.19945234 0.1990623  0.2011739  0.20041107 0.20048621 0.20102806\n",
      "  0.20071213 0.20041916 0.20153853 0.20101057]]\n",
      "['man', 'im', 'so', 'sad', 'school', 'is', 'ending', 'but', 'then', 'again', 'high', 'school', 'might', 'be', 'better', ':O']\n",
      "neutral\n"
     ]
    }
   ],
   "source": [
    "print(result)\n",
    "print(test_list[125])\n",
    "print(test_sentiment_list[125])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
